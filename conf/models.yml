model: &std_gpt2
    type: gpt2
    n_positions: 101
    n_layer: 12
    n_embd: 256
    n_head: 8

model: &std_llama
    type: llama
    n_positions: 101
    n_layer: 12
    n_embd: 256
    n_head: 8

model: &lora_std_gpt2
    type: lora
    base_model: *std_gpt2
    lora_config:
        r: 16
        lora_alpha: 16
        bias: none
        fan_in_fan_out: True
        lora_dropout: 0.0
        target_modules: ["attn.c_attn", "mlp.c_fc", "mlp.c_proj"]

model: &lora_std_llama
    type: lora
    base_model: *std_llama
    lora_config:
        r: 16
        lora_alpha: 16
        bias: none
        fan_in_fan_out: False
        lora_dropout: 0.0
        target_modules: ["self_attn.q_proj", "self_attn.k_proj", "self_attn.v_proj", "self_attn.o_proj"]

model: &knn_3
    type: knn
    n_neighbors: 3

model: &least_squares
    type: least squares

model: &lasso
    type: lasso
    alpha: 0.1

model: &d_tree
    type: decision tree

model: &xgboost
    type: xgboost

model: &averaging
    type: averaging

model: &decision_tree_4
    type: decision tree
    max_depth: 4

model: &decision_tree
    type: decision tree
    max_depth: !!null

model: &zero
    type: zero

model: &mlp
    type: grad mlp
    model_class_name: mlp
    model_class_args:
        dimensions: [100]
    opt_alg_name: adam
    batch_size: 100
    lr: !!float 5e-3
    num_steps: 100

model: &lasso_set
  - <<: *least_squares
  - <<: *averaging
  - <<: *lasso
    alpha: 0.0001
  - <<: *lasso
    alpha: 0.001
  - <<: *lasso
    alpha: 0.01
  - <<: *lasso
    alpha: 0.1
  - <<: *lasso
    alpha: 1
