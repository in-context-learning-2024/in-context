info: &hybrid_config
    n_positions: 101
    n_layer: 12
    n_embd: 256
    n_head: 8

model: &mambafirstgpt2_no_pos_embed
    type: mambafirstgpt2
    <<: *hybrid_config
    want_pos_embeddings: False
    no_attention: False
    num_mamba_layers: 1
    custom_attn_func: "vanilla"

model: &mambafirstgpt2_pos_embed
    type: mambafirstgpt2
    <<: *hybrid_config
    want_pos_embeddings: True
    no_attention: False
    num_mamba_layers: 4
    custom_attn_func: "vanilla"

model: &mambaonly_pos_embed
    type: mambaonly
    <<: *hybrid_config
    want_pos_embeddings: True
    no_attention: True
    num_mamba_layers: 10
    custom_attn_func: "vanilla"

model: &mambaonly_no_pos_embed
    type: mambaonly
    <<: *hybrid_config
    want_pos_embeddings: False
    no_attention: True
    num_mamba_layers: 5
    custom_attn_func: "vanilla"


model: &mambaformer_classic
    type: mambaformer_classic
    <<: *hybrid_config
    want_pos_embeddings: False
    no_attention: False
    num_mamba_layers: 1
    num_mamba_instances: 2
    custom_attn_func: "vanilla"

model: &mod_transformer
    type: mod_transformer
    <<: *hybrid_config
    want_pos_embeddings: True
    no_attention: False
    custom_attn_func: "relu"

model: &llama_mamba_classic
    type: llama_mamba
    <<: *hybrid_config
    want_rope: True
    hidden_act: "silu"
    rope_theta: 1000
    num_mamba_layers: 1
    num_mamba_instances: 2

model: &llama_mamba_no_rope
    type: llama_mamba
    <<: *hybrid_config
    want_rope: False
    hidden_act: "silu"
    rope_theta: 1000
    num_mamba_layers: 1
    num_mamba_instances: 2

model: &llama_gelu_no_rope
    type: llama_mod
    <<: *hybrid_config
    want_rope: False
    hidden_act: "gelu_new"
    rope_theta: 1000
    
model: &llama_no_rope
    type: llama_mod
    <<: *hybrid_config
    want_rope: False
    hidden_act: "silu"
    rope_theta: 1000
    
model: &llama_gelu
    type: llama_mod
    <<: *hybrid_config
    want_rope: True
    hidden_act: "gelu_new"
    rope_theta: 1000

model: &llama_mamba_over_ffn
    type: llama_standard_hybrid
    <<: *hybrid_config
    want_rope: True
    hidden_act: "silu"
    rope_theta: 1000
    num_mamba_layers: 1
    num_mamba_instances: 1
